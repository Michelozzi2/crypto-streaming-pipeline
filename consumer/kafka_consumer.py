"""
Consumer Kafka pour le pipeline crypto streaming
Consomme les messages de Kafka et les stocke dans PostgreSQL
"""
import json
import logging
import signal
import sys
import time
from typing import List, Dict
from datetime import datetime

from kafka import KafkaConsumer
from kafka.errors import KafkaError

# Import de nos modules
import config
from db_manager import DatabaseManager

# Configuration du logging
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('consumer.log')
    ]
)
logger = logging.getLogger(__name__)


class CryptoKafkaConsumer:
    """
    Consumer Kafka pour les donnÃ©es crypto
    """
    
    def __init__(self):
        """Initialise le consumer"""
        self.running = False
        self.consumer = None
        self.db_manager = None
        
        # Statistiques
        self.messages_consumed = 0
        self.messages_stored = 0
        self.errors = 0
        self.start_time = None
        
        # Buffer pour le batch processing
        self.message_buffer: List[Dict] = []
    
    def setup(self):
        """Configure le consumer Kafka et la base de donnÃ©es"""
        try:
            logger.info("ğŸš€ DÃ©marrage du Consumer Crypto Kafka")
            logger.info("=" * 60)
            
            # 1. Connexion Ã  PostgreSQL
            logger.info("ğŸ’¾ Connexion Ã  PostgreSQL...")
            self.db_manager = DatabaseManager(
                host=config.POSTGRES_HOST,
                port=config.POSTGRES_PORT,
                database=config.POSTGRES_DB,
                user=config.POSTGRES_USER,
                password=config.POSTGRES_PASSWORD
            )
            
            if not self.db_manager.connect():
                raise Exception("Impossible de se connecter Ã  PostgreSQL")
            
            # 2. Initialiser le consumer Kafka
            logger.info("ğŸ”Œ Connexion Ã  Kafka...")
            self.consumer = KafkaConsumer(
                config.KAFKA_TOPIC_RAW,
                bootstrap_servers=config.KAFKA_BOOTSTRAP_SERVERS,
                
                # Consumer group (pour la scalabilitÃ©)
                group_id=config.KAFKA_CONSUMER_GROUP,
                
                # DÃ©sÃ©rialisation
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                
                # Configuration de consommation
                auto_offset_reset='earliest',  # Commence au dÃ©but si nouveau consumer
                enable_auto_commit=config.CONSUMER_ENABLE_AUTO_COMMIT,
                max_poll_records=config.CONSUMER_MAX_POLL_RECORDS,
                
                # Timeouts
                session_timeout_ms=30000,
                heartbeat_interval_ms=10000,
                max_poll_interval_ms=300000  # 5 minutes
            )
            
            logger.info("âœ… Consumer Kafka initialisÃ©")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š Configuration:")
            logger.info(f"   â€¢ Kafka: {config.KAFKA_BOOTSTRAP_SERVERS}")
            logger.info(f"   â€¢ Topic: {config.KAFKA_TOPIC_RAW}")
            logger.info(f"   â€¢ Consumer Group: {config.KAFKA_CONSUMER_GROUP}")
            logger.info(f"   â€¢ Batch Size: {config.BATCH_SIZE}")
            logger.info(f"   â€¢ PostgreSQL: {config.POSTGRES_HOST}:{config.POSTGRES_PORT}")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'initialisation: {e}")
            return False
    
    def process_message(self, message) -> bool:
        """
        Traite un message Kafka
        
        Args:
            message: Message Kafka
            
        Returns:
            True si traitÃ© avec succÃ¨s
        """
        try:
            # Extraire les donnÃ©es
            data = message.value
            key = message.key
            
            logger.debug(
                f"ğŸ“¨ Message reÃ§u: {key} "
                f"(partition {message.partition}, offset {message.offset})"
            )
            
            # PrÃ©parer les donnÃ©es pour PostgreSQL
            db_data = {
                'symbol': data.get('symbol', 'UNKNOWN'),
                'price_usd': float(data.get('price_usd', 0)),
                'volume_24h': float(data.get('volume_24h', 0)),
                'market_cap_usd': float(data.get('market_cap_usd', 0)),
                'change_percent_24h': float(data.get('change_percent_24h', 0)),
                'timestamp': int(data.get('timestamp', 0))
            }
            
            # Ajouter au buffer
            self.message_buffer.append(db_data)
            self.messages_consumed += 1
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur traitement message: {e}")
            self.errors += 1
            return False
    
    def flush_buffer(self) -> bool:
        """
        Vide le buffer en base de donnÃ©es (batch insert)
        
        Returns:
            True si succÃ¨s
        """
        if not self.message_buffer:
            return True
        
        try:
            # Insertion en batch
            rows_inserted = self.db_manager.insert_crypto_data_batch(
                self.message_buffer
            )
            
            self.messages_stored += rows_inserted
            
            # Vider le buffer
            self.message_buffer.clear()
            
            # Commit Kafka (on a bien traitÃ© ces messages)
            if not config.CONSUMER_ENABLE_AUTO_COMMIT:
                self.consumer.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur flush buffer: {e}")
            self.errors += 1
            return False
    
    def run(self):
        """
        Boucle principale du consumer
        """
        if not self.setup():
            logger.error("âŒ Ã‰chec de l'initialisation")
            return
        
        self.running = True
        self.start_time = time.time()
        
        logger.info("ğŸƒ DÃ©marrage de la consommation")
        logger.info("   Appuie sur Ctrl+C pour arrÃªter\n")
        
        try:
            # Boucle de consommation
            for message in self.consumer:
                if not self.running:
                    break
                
                # Traiter le message
                self.process_message(message)
                
                # Flush le buffer si on atteint la taille de batch
                if len(self.message_buffer) >= config.BATCH_SIZE:
                    self.flush_buffer()
                    
                    # Log des stats toutes les X insertions
                    if self.messages_stored % (config.BATCH_SIZE * 5) == 0:
                        self.print_stats()
        
        except KeyboardInterrupt:
            logger.info("\n\nâš ï¸  Interruption clavier dÃ©tectÃ©e")
        except Exception as e:
            logger.error(f"âŒ Erreur fatale: {e}")
        finally:
            # Flush les messages restants
            if self.message_buffer:
                logger.info(f"ğŸ“¤ Flush des {len(self.message_buffer)} messages restants...")
                self.flush_buffer()
            
            self.shutdown()
    
    def print_stats(self):
        """Affiche les statistiques"""
        if self.start_time:
            duration = time.time() - self.start_time
            rate_consumed = self.messages_consumed / duration if duration > 0 else 0
            rate_stored = self.messages_stored / duration if duration > 0 else 0
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š Statistiques temps rÃ©el:")
            logger.info(f"   â€¢ Messages consommÃ©s: {self.messages_consumed}")
            logger.info(f"   â€¢ Messages stockÃ©s: {self.messages_stored}")
            logger.info(f"   â€¢ Buffer actuel: {len(self.message_buffer)}")
            logger.info(f"   â€¢ Erreurs: {self.errors}")
            logger.info(f"   â€¢ Taux consommation: {rate_consumed:.2f} msg/s")
            logger.info(f"   â€¢ Taux stockage: {rate_stored:.2f} msg/s")
            logger.info(f"   â€¢ DurÃ©e: {duration:.2f}s")
            logger.info("=" * 60)
    
    def shutdown(self):
        """ArrÃªt propre du consumer"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ›‘ ArrÃªt du consumer...")
        
        self.running = False
        
        # Statistiques finales
        self.print_stats()
        
        # Statistiques base de donnÃ©es
        if self.db_manager:
            logger.info("\nğŸ“Š Statistiques base de donnÃ©es:")
            stats = self.db_manager.get_statistics()
            if stats:
                logger.info(f"   â€¢ Total lignes: {stats.get('total_rows', 0)}")
                logger.info(f"   â€¢ Cryptos uniques: {stats.get('total_cryptos', 0)}")
                logger.info(f"   â€¢ PremiÃ¨re insertion: {stats.get('first_insert', 'N/A')}")
                logger.info(f"   â€¢ DerniÃ¨re insertion: {stats.get('last_insert', 'N/A')}")
        
        # Fermer les connexions
        if self.consumer:
            logger.info("ğŸ”Œ Fermeture du consumer Kafka...")
            self.consumer.close()
        
        if self.db_manager:
            self.db_manager.close()
        
        logger.info("âœ… ArrÃªt terminÃ©")
        logger.info("=" * 60)


def signal_handler(sig, frame):
    """Gestion des signaux systÃ¨me (Ctrl+C)"""
    logger.info("\nâš ï¸  Signal reÃ§u, arrÃªt en cours...")
    sys.exit(0)


def main():
    """Point d'entrÃ©e principal"""
    # GÃ©rer Ctrl+C proprement
    signal.signal(signal.SIGINT, signal_handler)
    
    # CrÃ©er et lancer le consumer
    consumer = CryptoKafkaConsumer()
    consumer.run()


if __name__ == "__main__":
    main()
