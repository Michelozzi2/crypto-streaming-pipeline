# ğŸ‰ SETUP TERMINÃ‰ - Guide de dÃ©marrage

## âœ… Ce qui a Ã©tÃ© crÃ©Ã©

### 1. Infrastructure Docker (docker-compose.yml)
- **Kafka + Zookeeper** : Pour le streaming de messages
- **Kafka UI** : Interface web pour visualiser Kafka
- **PostgreSQL** : Base de donnÃ©es pour donnÃ©es brutes
- **ClickHouse** : Base columnar pour time-series
- **Redis** : Cache in-memory
- **Grafana** : Dashboards et visualisation
- **Prometheus** : Monitoring et mÃ©triques

### 2. Scripts d'initialisation
- **init-db.sql** : CrÃ©e les tables PostgreSQL
  - crypto_raw : donnÃ©es brutes
  - crypto_aggregated_5m : agrÃ©gations 5 minutes
  - crypto_alerts : alertes de variations
  - crypto_moving_averages : moyennes mobiles
  - crypto_metadata : mÃ©tadonnÃ©es des cryptos

- **clickhouse-init.sql** : CrÃ©e les tables ClickHouse
  - crypto.prices : donnÃ©es time-series
  - crypto.ohlc_5m : agrÃ©gations OHLC 5 minutes
  - crypto.ohlc_1h : agrÃ©gations OHLC 1 heure
  - Vues matÃ©rialisÃ©es pour calculs automatiques

### 3. Configuration
- **prometheus.yml** : Config monitoring
- **.env** : Variables d'environnement
- **requirements.txt** : DÃ©pendances Python
- **.gitignore** : Fichiers Ã  ignorer
- **README.md** : Documentation du projet

## ğŸš€ PROCHAINES Ã‰TAPES

### Ã‰tape 1 : DÃ©marrer l'infrastructure (5 min)

```bash
# 1. Ouvre un terminal dans le dossier du projet
cd crypto-streaming-pipeline

# 2. Lance Docker Desktop (ou Docker Engine)

# 3. DÃ©marre tous les services
docker-compose up -d

# 4. VÃ©rifie que tout est bien lancÃ©
docker-compose ps

# Attends 30-60 secondes que tout soit prÃªt
```

### Ã‰tape 2 : VÃ©rifier les interfaces web (2 min)

Ouvre ces URLs dans ton navigateur :

1. **Kafka UI** : http://localhost:8080
   - Tu devrais voir l'interface Kafka (pas de topics pour l'instant, c'est normal)

2. **Grafana** : http://localhost:3000
   - Login : admin / admin
   - Tu peux changer le mot de passe ou skip

3. **Prometheus** : http://localhost:9090
   - Interface de monitoring

4. **ClickHouse** : http://localhost:8123
   - Tu devrais voir "Ok."

### Ã‰tape 3 : Setup Python (3 min)

```bash
# 1. CrÃ©e un environnement virtuel Python
python -m venv venv

# 2. Active l'environnement
# Sur Mac/Linux :
source venv/bin/activate
# Sur Windows :
venv\Scripts\activate

# 3. Installe les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“ Ce qu'on va faire MAINTENANT

### Prochaine session : CrÃ©er le Kafka Producer

On va crÃ©er le script Python qui :
1. Se connecte Ã  l'API CoinCap
2. RÃ©cupÃ¨re les prix de 10 cryptos toutes les 10 secondes
3. Envoie les donnÃ©es dans Kafka

**Fichiers Ã  crÃ©er :**
- `producer/kafka_producer.py` : Producer principal
- `producer/api_client.py` : Client pour l'API CoinCap
- `producer/config.py` : Configuration

**Ce que tu vas apprendre :**
- Comment se connecter Ã  une API REST
- Comment produire des messages dans Kafka
- Gestion des erreurs et retry logic
- Monitoring avec Prometheus

## ğŸ“ Concepts importants Ã  retenir

### Kafka
- **Topic** : Canal de messages (comme une file d'attente)
- **Producer** : Envoie des messages dans un topic
- **Consumer** : Lit les messages d'un topic
- **Broker** : Serveur Kafka qui gÃ¨re les topics

### Architecture
```
API â†’ Producer â†’ Kafka Topic â†’ Consumer â†’ Database
```

### Notre flux
```
CoinCap API â†’ Python Producer â†’ Kafka Topic "crypto-prices-raw"
                                       â†“
                                  Consumer â†’ PostgreSQL
```

## âš ï¸ ProblÃ¨mes courants

### Docker ne dÃ©marre pas
```bash
# VÃ©rifie que Docker est lancÃ©
docker ps

# RedÃ©marre Docker Desktop
```

### Port dÃ©jÃ  utilisÃ© (ex: 5432, 9092)
```bash
# Trouve le processus qui utilise le port
# Mac/Linux :
lsof -i :5432
# Windows :
netstat -ano | findstr :5432

# ArrÃªte le processus ou change le port dans docker-compose.yml
```

### Python venv ne fonctionne pas
```bash
# Assure-toi d'avoir Python 3.9+
python --version

# Sur Windows, tu dois peut-Ãªtre faire :
python -m pip install --upgrade pip
```

## ğŸ“Š Status actuel

```
âœ… Infrastructure Docker configurÃ©e
âœ… Bases de donnÃ©es initialisÃ©es
âœ… Monitoring en place
âœ… Configuration prÃªte

ğŸš§ Ã€ venir (prochaine session) :
   - Producer Kafka
   - Consumer Kafka
   - Spark Streaming
```

## ğŸ’¡ Conseils

1. **Prends le temps** : Ne rush pas, comprends chaque Ã©tape
2. **Teste rÃ©guliÃ¨rement** : VÃ©rifie que chaque composant fonctionne
3. **Lis les logs** : `docker-compose logs -f nom_service`
4. **Documente** : Note ce que tu apprends dans un fichier NOTES.md

## ğŸ¯ Objectif de la semaine 1

Ã€ la fin de la semaine 1, tu auras :
- âœ… Infrastructure complÃ¨te qui tourne
- ğŸ¯ Producer qui envoie des donnÃ©es dans Kafka
- ğŸ¯ Consumer qui stocke dans PostgreSQL
- ğŸ¯ Dashboard Kafka UI qui montre le flux

## ğŸ“ Besoin d'aide ?

Si quelque chose ne marche pas :
1. VÃ©rifie les logs : `docker-compose logs nom_service`
2. VÃ©rifie que Docker a assez de RAM (8GB minimum)
3. RedÃ©marre les services : `docker-compose restart`
4. En dernier recours : `docker-compose down -v && docker-compose up -d`

---

**ğŸ‰ FÃ‰LICITATIONS !** Tu as configurÃ© une infrastructure data engineering professionnelle !

**PrÃªt pour la suite ?** Dis-moi quand tu veux qu'on code le Producer Kafka ! ğŸš€
